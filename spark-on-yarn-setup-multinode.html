<!DOCTYPE html>
<html lang="zh-Hans,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.mak-blog.com","root":"/","scheme":"Muse","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="环境：Ubuntu 12.04 LTS Desktop 64bit 提示：这里只用了一台机器同时做master节点和slave节点，如果想要分布式部署，即多个slave节点，准备环境部分一样，然后配置文档做一些修改增加slave节点就行，看完这份文档就知道怎么修改了">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark on YARN分布式部署文档">
<meta property="og:url" content="http://www.mak-blog.com/spark-on-yarn-setup-multinode.html">
<meta property="og:site_name" content="Songtao Mai">
<meta property="og:description" content="环境：Ubuntu 12.04 LTS Desktop 64bit 提示：这里只用了一台机器同时做master节点和slave节点，如果想要分布式部署，即多个slave节点，准备环境部分一样，然后配置文档做一些修改增加slave节点就行，看完这份文档就知道怎么修改了">
<meta property="article:published_time" content="2015-03-09T17:31:01.000Z">
<meta property="article:modified_time" content="2020-02-20T09:14:51.559Z">
<meta property="article:author" content="Songtao Mai">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="YARN">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://www.mak-blog.com/spark-on-yarn-setup-multinode.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Spark on YARN分布式部署文档 | Songtao Mai</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Songtao Mai</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right"></div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://www.mak-blog.com/spark-on-yarn-setup-multinode.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Songtao Mai">
      <meta itemprop="description" content="记录，分享，交流的空间">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Songtao Mai">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark on YARN分布式部署文档
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-03-10 01:31:01" itemprop="dateCreated datePublished" datetime="2015-03-10T01:31:01+08:00">2015-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-20 17:14:51" itemprop="dateModified" datetime="2020-02-20T17:14:51+08:00">2020-02-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index">
                    <span itemprop="name">科研</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>环境：Ubuntu 12.04 LTS Desktop 64bit 提示：这里只用了一台机器同时做master节点和slave节点，如果想要分布式部署，即多个slave节点，准备环境部分一样，然后配置文档做一些修改增加slave节点就行，看完这份文档就知道怎么修改了</p>
<a id="more"></a>

<h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><p>我这里全程都是不用root模式的。</p>
<ol>
<li>设置用户名 我这里设成master</li>
<li>配置hosts文档 <code>vi /etc/hosts</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.1.1.107      master</span><br><span class="line">10.1.1.108      slave1</span><br><span class="line">10.1.1.109      slave2</span><br></pre></td></tr></table></figure>
配置之后ping一下用户名看是否生效<br><code>ping master</code></li>
<li>关闭防火墙<br><code>sudo ufw disable</code></li>
<li>安装JAVA 提示：最好建一个目录，把需要用到的东西都装在这一个目录里面，这样比较方便，我用的是/work，从官网下载最新版JAVA就可以，Spark官方说明JAVA只要是6以上的版本都可以，我下的是jdk-7u75-linux-x64.gz 在/work这目录下直接解压<br><code>tar xvzf jdk-7u75-linux-x64.gz</code><br>修改环境变量 vi /etc/profile 添加下列内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA\_HOME&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;jdk1.7.0\_75</span><br><span class="line">export JRE\_HOME&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;jdk1.7.0\_75&#x2F;jre</span><br><span class="line">export PATH&#x3D;$JAVA\_HOME&#x2F;bin:$JAVA\_HOME&#x2F;jre&#x2F;bin:$PATH</span><br><span class="line">export CLASSPATH&#x3D;$CLASSPATH:.:$JAVA\_HOME&#x2F;lib:$JAVA\_HOME&#x2F;jre&#x2F;lib</span><br></pre></td></tr></table></figure>
使环境变量生效<br><code>source /etc/profile</code><br>验证一下JAVA是否成功安装<br><code>java -version</code><br>如果打印出版本信息，说明安装成功 <a href="http://www.windowskeys.net" target="_blank" rel="noopener">windowskeys.net</a> <a href="http://www.windowskeys.net" target="_blank" rel="noopener">buy windows 10 key</a></li>
<li>安装配置Scala Spark官方说明Scala要求为2.10.x版本，注意不要下错版本，我这里下了2.10.3 同样放在/usr/java目录下然后解压<br><code>tar xvzf scala-2.10.3.tgz</code><br>修改环境变量 vi /etc/profile 添加下列内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;scala-2.10.3</span><br><span class="line">export PATH&#x3D;$PATH:$SCALA_HOME&#x2F;bin</span><br></pre></td></tr></table></figure>
使环境变量生效<br><code>source /etc/profile</code><br>验证Scala是否安装成功<br><code>scala -version</code><br>如果打印出来版本信息，说明安装成功 0.6 配置SSH无密码通信 安装Openssh server<br><code>apt-get install openssh-server</code><br>我这里没联网，所以就在<code>http://www.openssl.org/source/openssl-1.0.1e.tar.gz</code>下安装包编译。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar xvzf openssl-1.0.1e.tar.gz</span><br><span class="line">cd openssl-1.0.1e</span><br><span class="line">.&#x2F;config shared --prefix&#x3D;&#x2F;usr&#x2F;local</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>
在所有机器上都生成私钥和公钥<br><code>ssh-keygen -t rsa （然后一直按回车）</code><br>如果要机器间都能相互访问，要把所有机器都公钥都拷到authorized_keys，传输公钥可以用scp来传输。<br><code>scp .ssh/id\_rsa.pub administrator@slave1:/home/administrator/.ssh/id\_rsa_master.pub</code><br>注意提示的密钥所在的目录，然后切换到那个目录里面，然后把所有的公钥都添加进authorized_keys<br><code>cat .ssh/id\_rsa.pub &gt;&gt; .ssh/authorized\_keys</code><br>验证SSH无密码通信<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh master</span><br><span class="line">ssh slave1</span><br><span class="line">ssh slave2</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="hadoop-YARN安装"><a href="#hadoop-YARN安装" class="headerlink" title="hadoop YARN安装"></a>hadoop YARN安装</h3><ol>
<li>安装hadoop 从官网下载hadoop2.6.0版本 解压<br><code>tar xvzf hadoop-2.6.0.tar.gz</code><br>hadoop-2.6.0不应该解压到/usr/java，因为/usr/java需要root权限才能访问，最好不要用到root权限。 解压之后会在~/work目录下，看到hadoop的文档：hadoop-2.6.0</li>
<li>配置hadoop 在hadoop-2.6.0/etc/hadoop/hadoop-env.sh中，添加JAVA安装目录<br><code>export JAVA\_HOME=//home/administrator/work/jdk1.7.0\_75</code><br>在hadoop-2.6.0/etc/hadoop下，将mapred-site.xml.templat重命名成mapred-site.xml，并添加以下内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
在hadoop-2.6.0/etc/hadoop/中，修改core-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.default.name&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hdfs:&#x2F;&#x2F;master:8020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;final&gt;true&lt;&#x2F;final&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
在hadoop-2.6.0/etc/hadoop/中，修改yarn-site.xml：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;master:8030&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;master:8031&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;master:8032&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.admin.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;master:8033&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
注：之前从网上看到的配置文档都是mapreduce.shuffle，然后起yarn的时候yarn就一直没起起来，运行jps命令，可以看到yarn起起来，而且有进程号，但是其实已经挂了，去看log文档发现有错，具体错误忘记截图留念了 修改hdfs-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;mnt&#x2F;disk1&#x2F;yarn&#x2F;dfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;mnt&#x2F;disk1&#x2F;yarn&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.permissions&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
在hadoop-2.6.0/etc/hadoop/slaves文件中添加你的节点ip或者host：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
修改bashrc文件 vi /home/administrator/.bashrc 添加以下内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP\_COMMON\_LIB\_NATIVE\_DIR&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;hadoop-2.6.0&#x2F;lib&#x2F;native</span><br><span class="line">export HADOOP_OPTS&#x3D;&quot;-Djava.library.path&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;hadoop-2.6.0&#x2F;lib&quot;</span><br><span class="line">export HADOOP\_ROOT\_LOGGER&#x3D;DEBUG,console</span><br><span class="line">&#x2F;&#x2F;开启debug调试</span><br></pre></td></tr></table></figure>
<code>source /home/administrator/.bashrc</code><br>通常编译好的hadoop库是在lib中，如果你不想编译，可以用lib/native里面的预编译库，然后把native的库移动到lib文件夹中。<br><code>cp hadoop-2.6.0/lib/native/* hadoop-2.6.0/lib/</code></li>
<li>验证hadoop是否安装成功 启动HDFS 首次启动集群时，做如下操作【主名字节点上执行】<br><code>bin/hadoop namenode -format</code><br>进入hadoop所在目录<br><code>sbin/start-dfs.sh</code><br>启动YARN<br><code>sbin/start-yarn.sh</code><br>打开浏览器，输入<a href="http://master:8088" target="_blank" rel="noopener">http://master:8088</a> 这里有一个大坑，如果运行上面的format命令时，会问要不要清空原来的文件，如果清空的话，第二次启动是datanode死活都启动不起来，因为那时候slave节点的文件id是从master节点复制过来的，要保持一致才能打开datanode节点，如果format之后，master节点的文件id就回重新生成，所以这时候要不就不清空文件；如果清空文件就要把datanode的文件也顺便手动删除一下。</li>
<li>在YARN下尝试运行MapReduce例子程序 搭好hadoop之后，想尝试跑一下例子程序，发现遇到8020端口连接失败错误 网上各种方法都尝试了，包括更改core_site.xml里面的端口为8020，检查防火墙，网络配置都没有问题 后来一个一个log看，发现是/mnt/disk1/yarn/dfs/name 文件有问题 把hdfs格式化久可以了<br><code>bin/hadoop namenode -format</code><br>运行pi例子程序<br><code>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar pi 20 10</code><br>命令里的20 10分别指20个map10个reduce</li>
</ol>
<h3 id="Spark安装"><a href="#Spark安装" class="headerlink" title="Spark安装"></a>Spark安装</h3><ol>
<li>安装Spark 从官网下载最新版Spark 我下的是spark-1.2.1-bin-hadoop2.4.tgz 解压<code>tar xvzf spark-1.2.1-bin-hadoop2.4.tgz</code></li>
<li>配置Spark vi conf/spark-env.sh 添加一下内容<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export SPARK\_LOCAL\_IP&#x3D;根据实际情况填写</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;scala-2.10.3</span><br><span class="line">export JAVA\_HOME&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;jdk1.7.0\_75</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;hadoop-2.6.0</span><br><span class="line">export SPARK\_LOCAL\_DIR&#x3D;&#x2F;home&#x2F;administrator&#x2F;work&#x2F;spark-1.2.1-bin-hadoop2.4</span><br><span class="line">export SPARK\_JAVA\_OPTS&#x3D;&quot;-Dspark.storage.blockManagerHeartBeatMs&#x3D;60000 -Dspark.local.dir&#x3D;$SPARK\_LOCAL\_DIR -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:$SPARK_HOME&#x2F;logs&#x2F;gc.log -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction&#x3D;60 -Xms256m -Xmx256m -XX:MaxPermSize&#x3D;128m&quot;</span><br><span class="line">export SPARK\_MASTER\_IP&#x3D;master</span><br><span class="line">export SPARK\_MASTER\_PORT&#x3D;7077</span><br><span class="line">export SPARK\_WORKER\_CORES&#x3D;1</span><br><span class="line">export SPARK\_WORKER\_MEMORY&#x3D;2g</span><br><span class="line">export SPARK\_WORKER\_PORT&#x3D;9090</span><br><span class="line">export SPARK\_WORKER\_WEBUI_PORT&#x3D;9099</span><br><span class="line">export HADOOP\_CONF\_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure>
在slave文件下填上slave主机名：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></li>
<li>启动Spark<br><code>sbin/start-all.sh</code></li>
<li>验证Spark 输入命令jps，可以看到打出的JVM程序<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">8566 SecondaryNameNode</span><br><span class="line">8955 NodeManager</span><br><span class="line">8082 NameNode</span><br><span class="line">17022 Jps</span><br><span class="line">8733 ResourceManager</span><br><span class="line">8296 DataNode</span><br></pre></td></tr></table></figure>
进入Spark的Web管理页面：<br><code>master:8080</code><br>运行命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#本地模式两线程运行</span><br><span class="line">.&#x2F;bin&#x2F;run-example SparkPi 10 --master local\[2\]</span><br><span class="line"></span><br><span class="line">\# Run application locally on 8 cores</span><br><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master local\[6\] \</span><br><span class="line">  lib&#x2F;spark-examples-1.2.1-hadoop2.4.0.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line">\# Run on a Spark Standalone cluster in client deploy mode</span><br><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark:&#x2F;&#x2F;master:7077 \</span><br><span class="line">  --executor-memory 8G \</span><br><span class="line">  --total-executor-cores 4 \</span><br><span class="line">  lib&#x2F;spark-examples-1.2.1-hadoop2.4.0.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line">\# Run on a Spark Standalone cluster in cluster deploy mode with supervise</span><br><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark:&#x2F;&#x2F;master:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 8G \</span><br><span class="line">  --total-executor-cores 8 \</span><br><span class="line">  lib&#x2F;spark-examples-1.2.1-hadoop2.4.0.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line">\# Run on a YARN cluster</span><br><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  # can also be \&#96;yarn-client\&#96; for client mode</span><br><span class="line">  --executor-memory 8G \</span><br><span class="line">  --num-executors 8 \</span><br><span class="line">  &#x2F;path&#x2F;to&#x2F;examples.jar \</span><br><span class="line">  100</span><br></pre></td></tr></table></figure></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
              <a href="/tags/YARN/" rel="tag"># YARN</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"># 分布式</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/sfbuy-adidas-high-top-shoe.html" rel="prev" title="用SFBuy顺丰海淘阿迪高帮鞋">
      <i class="fa fa-chevron-left"></i> 用SFBuy顺丰海淘阿迪高帮鞋
    </a></div>
      <div class="post-nav-item">
    <a href="/ubuntu1204-eclipse-tomcat-jdk.html" rel="next" title="Ubuntu 12.04 Eclipse-Tomcat搭建">
      Ubuntu 12.04 Eclipse-Tomcat搭建 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备环境"><span class="nav-number">1.</span> <span class="nav-text">准备环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop-YARN安装"><span class="nav-number">2.</span> <span class="nav-text">hadoop YARN安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark安装"><span class="nav-number">3.</span> <span class="nav-text">Spark安装</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Songtao Mai</p>
  <div class="site-description" itemprop="description">记录，分享，交流的空间</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">173</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Songtao Mai</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
